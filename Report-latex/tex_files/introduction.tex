\section{Introduction}

Data visualization is the process of representing data in a graphical or pictorial form which makes the information easy to understand.  Human beings have always employed visualizations to make messages or information last in time. What cannot be touched, smelled, or tasted can be represented visually \cite{sancho2014approach}.
Although there are multiple goals and objectives for visualizing data the main goal is to make something complex appear simple, especially for the untrained eye. Turning data into knowledge would mean different things to different people and would require a different process for different cases, but in all such cases, visualization offers an indispensable tool. Data visualization is useful for data cleaning, exploring data structure, identifying trends and clusters, detecting outliers and unusual groups, spotting local patterns, evaluating modeling output, and presenting results.

Graphics are a good way to raise questions and suggest ideas that help to stimulate research.
In fact, graphics reveal data features that statistics and models may miss. They reveal unusual distributions of data, local patterns, clusterings, gaps, missing values, evidence of rounding or heaping, implicit boundaries, outliers, and so on.  New, innovative graphics need instruction and experience to interpret them. Their designers have spent much time developing them and reasonably enough believe that what is obvious to them should be obvious to everyone.  To visualize data many questions need to be answered. What are the reasons behind drawing such displays, what is the context, where did the data come from, and how much data is needed? 


 
 \\\

\subsection{Interactive Visualization and Big Data}

One picture is worth a thousand words. In today's world where data is being recorded from every click on the web to personal records, petabytes of data are being generated and processed every day. It is not enough to process and analyze those data since human brains are incapable of seeing all patterns without the data being visually represented. As big data visualization plays an important role in decision making in various fields, it's, however, challenging to visualize such a huge amount of data in real-time or in static form. Static visualizations are only capable of providing views of the data while a huge data set requires many static views to present a variety of perspectives on the same information. Due to the increased quantity and limited number of pixels in display devices, it is often impossible to visualize all the raw data.





Even if assuming a sufficient number of pixels are available, showing all the data in any single view might not be beneficial as visual perception may be hindered by visual crowding \cite{wolfe2015sensation}. One way to overcome such a problem is an approach often called the keyhole problem which is to show the full details of a small number of items in the visualization \cite{salvendy2012handbook}. For example, by showing a few rows at a time of a huge spreadsheet and giving the user the ability to scroll through the spreadsheet. The downside of such an approach is that the user inevitably loses context even if it could help manage a large amount of data.

\\\

 An enduring design strategy that was introduced by Shneiderman is “overview first, zoom and filter, then details on demand” \cite{shneiderman2003eyes}. This strategy came to avoid the keyhole problem and it works by beginning the analysis with a broad overview of the entire data set. Obviously, some details would be sacrificed by following this approach however interaction techniques are coupled with visualization to allow the user to zoom in on specific information and filter out irrelevant items. More details on this strategy and its approaches are in the next Section.

